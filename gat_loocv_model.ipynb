{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec37be-1573-484e-a11e-5bfa24fe70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GAT LOOCV MODEL - NEURAL GRAPH LEARNING FOR BEHAVIOR PREDICTION\n",
    "# ============================================================\n",
    "# Predicts animal-level freezing behavior using neuron-level features and \n",
    "# functional connectivity (edge weights) in a GAT neural network framework.\n",
    "# ============================================================\n",
    "\n",
    "# === Core Libraries ===\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Torch + GNN Frameworks ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, TransformerConv, global_mean_pool\n",
    "\n",
    "# === Preprocessing + Evaluation ===\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# === Optuna for Hyperparameter Tuning ===\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler\n",
    "\n",
    "# === Reproducibility (optional) ===\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8ff6f-b86f-43d4-9bae-03b21856ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neuron features, edge weights, and freezing labels\n",
    "def load_data(neuron_features_path, edge_weights_path, labels_path):\n",
    "    \"\"\"\n",
    "    Loads and validates neuron-level features, functional edge weights, and \n",
    "    animal-level freezing labels for graph-based behavioral prediction.\n",
    "\n",
    "    Depending on the GNN model used, edge weights may be incorporated into \n",
    "    attention mechanisms (if using an edge-aware model like GATWithEdgeAttr).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neuron_features_path : str\n",
    "        Path to Excel file containing neuron-level features for each animal.\n",
    "\n",
    "    edge_weights_path : str\n",
    "        Path to Excel file containing mean edge weights between neuron pairs \n",
    "        (used to construct edge_attr in GNN models if applicable).\n",
    "\n",
    "    labels_path : str\n",
    "        Path to Excel file containing animal-level behavioral labels (e.g., \n",
    "        Percentage Freezing) and experimental group.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    neuron_features_df : pd.DataFrame\n",
    "        DataFrame of neuron-level features with 'Unique ID' and 'Neuron' columns.\n",
    "\n",
    "    edge_weights_df : pd.DataFrame\n",
    "        DataFrame of neuron pairs and their corresponding edge weights. Required \n",
    "        for graph construction and optionally passed as edge_attr to the model.\n",
    "\n",
    "    labels_df : pd.DataFrame\n",
    "        DataFrame containing animal-level metadata including behavioral output \n",
    "        (freezing %) and group identifiers.\n",
    "    \"\"\"\n",
    "    neuron_features_df = pd.read_excel(neuron_features_path)\n",
    "    edge_weights_df = pd.read_excel(edge_weights_path)\n",
    "    labels_df = pd.read_excel(labels_path)\n",
    "\n",
    "    # Fix column naming inconsistencies\n",
    "    neuron_features_df.rename(columns={'n_clust': 'Neuron'}, inplace=True)\n",
    "\n",
    "    # Convert neuron pair strings like \"(1, 2)\" to tuples\n",
    "    def convert_pair(pair_str):\n",
    "        if 'dummy' in pair_str.lower():\n",
    "            return (0, 1)\n",
    "        nums = re.findall(r'\\d+', pair_str)\n",
    "        return tuple(map(int, nums))\n",
    "\n",
    "    edge_weights_df['Neuron Pair'] = edge_weights_df['Neuron Pair'].apply(convert_pair)\n",
    "\n",
    "    # Validation checks\n",
    "    assert set(neuron_features_df['Unique ID'].unique()).issubset(edge_weights_df['Unique ID'].unique()), \\\n",
    "        \"Mismatch in Unique IDs between neuron features and edge weights.\"\n",
    "    assert set(neuron_features_df['Unique ID'].unique()).issubset(labels_df['UniqueID'].unique()), \\\n",
    "        \"Mismatch in Unique IDs between neuron features and labels.\"\n",
    "\n",
    "    return neuron_features_df, edge_weights_df, labels_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b1d8f-f269-41a2-9fe0-63730030bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert per-animal data into PyG graph objects\n",
    "\n",
    "def create_graph_objects(neuron_features_df, edge_weights_df, labels_df):\n",
    "    \"\"\"\n",
    "    Constructs torch_geometric Data objects (one per animal) using:\n",
    "    - node features (neurons)\n",
    "    - edge index (connectivity)\n",
    "    - edge attributes (weights)\n",
    "    - label (freezing %)\n",
    "    - group and unique ID metadata\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    animal_graphs_first600 : list of torch_geometric.data.Data\n",
    "    animal_graphs_last600 : list of torch_geometric.data.Data\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    Both standard and edge-aware GAT models use the same graph format.\n",
    "    Only GATWithEdgeAttr will access the edge_attr during training.\n",
    "    \"\"\"\n",
    "\n",
    "    animal_graphs_first600 = []\n",
    "    animal_graphs_last600 = []\n",
    "\n",
    "    for unique_id in neuron_features_df['Unique ID'].unique():\n",
    "        neurons = neuron_features_df[neuron_features_df['Unique ID'] == unique_id]\n",
    "        edges = edge_weights_df[edge_weights_df['Unique ID'] == unique_id]\n",
    "        label_row = labels_df[labels_df['UniqueID'] == unique_id]\n",
    "\n",
    "        if neurons.empty or edges.empty or label_row.empty:\n",
    "            print(f\"Missing data for animal {unique_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        group = label_row['Group'].iloc[0]\n",
    "\n",
    "        # Feature selection\n",
    "        feat_f600 = ['Firing rate first 600', 'mISI (s) first 600', 'maxISI (s) first 600',\n",
    "                     'minISI (s) first 600', 'CVISI first 600', 'PC1score whole', 'PC2score whole']\n",
    "        feat_l600 = ['Firing rate last 600', 'mISI (s) last 600', 'maxISI (s) last 600',\n",
    "                     'minISI (s) last 600', 'CVISI last 600', 'PC1score whole', 'PC2score whole']\n",
    "\n",
    "        x_f600 = torch.tensor(neurons[feat_f600].values, dtype=torch.float)\n",
    "        x_l600 = torch.tensor(neurons[feat_l600].values, dtype=torch.float)\n",
    "\n",
    "        # Edge index (convert 1-indexed â†’ 0-indexed)\n",
    "        edge_index = torch.tensor([(i-1, j-1) for i, j in edges['Neuron Pair']], dtype=torch.long).t().contiguous()\n",
    "        edge_attr_f600 = torch.tensor(edges['Mean Edge Weight First 600s'].values, dtype=torch.float).view(-1, 1)\n",
    "        edge_attr_l600 = torch.tensor(edges['Mean Edge Weight Last 600s'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        y_f600 = torch.tensor(label_row['Percentage Freezing First 600'].values, dtype=torch.float)\n",
    "        y_l600 = torch.tensor(label_row['Percentage Freezing Last 600'].values, dtype=torch.float)\n",
    "\n",
    "        data_f600 = Data(x=x_f600, edge_index=edge_index, edge_attr=edge_attr_f600,\n",
    "                         y=y_f600, group=torch.tensor([group]), unique_id=torch.tensor([unique_id]))\n",
    "        data_l600 = Data(x=x_l600, edge_index=edge_index, edge_attr=edge_attr_l600,\n",
    "                         y=y_l600, group=torch.tensor([group]), unique_id=torch.tensor([unique_id]))\n",
    "\n",
    "        animal_graphs_first600.append(data_f600)\n",
    "        animal_graphs_last600.append(data_l600)\n",
    "\n",
    "    print(f\"Created {len(animal_graphs_first600)} graphs for First 600s\")\n",
    "    print(f\"Created {len(animal_graphs_last600)} graphs for Last 600s\")\n",
    "\n",
    "    return animal_graphs_first600, animal_graphs_last600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c9926-77c4-44f9-aca2-8bc3ce742407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety checks on edge index values\n",
    "\n",
    "def check_dummy_node_edges(graphs):\n",
    "    \"\"\"\n",
    "    Sanity check for edge index issues.\n",
    "    Any edge indices < 0 are corrected to the last valid node index.\n",
    "    \"\"\"\n",
    "    for graph in graphs:\n",
    "        eid = graph.unique_id.item()\n",
    "        if graph.edge_index.min() < 0:\n",
    "            print(f\"Edge index < 0 found in animal {eid}. Attempting fix.\")\n",
    "            corrected = graph.edge_index.clone()\n",
    "            corrected[corrected < 0] = graph.num_nodes - 1\n",
    "            graph.edge_index = corrected\n",
    "            print(f\"Edge correction applied for animal {eid}.\")\n",
    "        else:\n",
    "            print(f\"Edge index OK for animal {eid}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479ae2b-5f84-40ef-82bb-59550649930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to features\n",
    "\n",
    "def normalize_features(graphs):\n",
    "    \"\"\"\n",
    "    Applies sklearn StandardScaler to node features within each graph.\n",
    "    Normalizes features to zero mean and unit variance.\n",
    "\n",
    "    This is done graph-by-graph to preserve each animalâ€™s independence.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    This function only normalizes node features (graph.x).\n",
    "    Edge attributes (graph.edge_attr) are left unchanged.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    for graph in graphs:\n",
    "        graph.x = torch.tensor(scaler.fit_transform(graph.x), dtype=torch.float)\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3be24b-83d0-440f-a337-e438172d8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GAT Models with Optional Edge Attribute Support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, TransformerConv, global_mean_pool\n",
    "\n",
    "# ============================================\n",
    "# TOGGLE THIS TO SWITCH BETWEEN MODELS\n",
    "# ============================================\n",
    "use_edge_weights = True  #Set to False for standard GAT without edge_attr\n",
    "\n",
    "# ============================================\n",
    "# MODEL 1 â€” Standard GAT (no edge weights)\n",
    "# ============================================\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard Graph Attention Network using GATConv.\n",
    "    - Ignores edge weights\n",
    "    - Uses 3 attention layers with dropout\n",
    "    - Scales output to 0â€“100 (freezing %)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_channels, dropout_rate=0.3, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels * heads, 1, heads=1)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return torch.sigmoid(x) * 100\n",
    "\n",
    "# ============================================\n",
    "# MODEL 2 â€” Edge-Aware GAT (uses edge_attr)\n",
    "# ============================================\n",
    "class GATWithEdgeAttr(nn.Module):\n",
    "    \"\"\"\n",
    "    Edge-aware Graph Attention Network using TransformerConv.\n",
    "    - Accepts edge_attr (e.g., functional connectivity)\n",
    "    - 3 attention layers with dropout\n",
    "    - Scales output to 0â€“100 (freezing %)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_channels, dropout_rate=0.3, heads=1):\n",
    "        super(GATWithEdgeAttr, self).__init__()\n",
    "        self.conv1 = TransformerConv(num_features, hidden_channels, heads=heads, edge_dim=1)\n",
    "        self.conv2 = TransformerConv(hidden_channels * heads, hidden_channels, heads=heads, edge_dim=1)\n",
    "        self.conv3 = TransformerConv(hidden_channels * heads, 1, heads=1, edge_dim=1)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return torch.sigmoid(x) * 100\n",
    "\n",
    "# ============================================\n",
    "# Model Instantiation (based on toggle)\n",
    "# ============================================\n",
    "def create_model(num_features, hidden_channels, dropout_rate=0.3, heads=1):\n",
    "    \"\"\"\n",
    "    Creates either a standard GAT or an edge-aware GAT model\n",
    "    based on the use_edge_weights flag.\n",
    "    \"\"\"\n",
    "    if use_edge_weights:\n",
    "        return GATWithEdgeAttr(num_features, hidden_channels, dropout_rate, heads)\n",
    "    else:\n",
    "        return GAT(num_features, hidden_channels, dropout_rate, heads)\n",
    "\n",
    "# === EXAMPLE USAGE ===\n",
    "# model = create_model(num_features=6, hidden_channels=64, dropout_rate=0.2, heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fed4df-0000-474a-bd65-5bb74a4fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GAT or GATWithEdgeAttr Model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=110):\n",
    "    \"\"\"\n",
    "    Trains the GAT model (with or without edge weights) and records loss per epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        Either GAT (no edge_attr) or GATWithEdgeAttr (uses edge_attr).\n",
    "    train_loader : DataLoader\n",
    "        Training graph batch loader.\n",
    "    val_loader : DataLoader\n",
    "        Validation graph batch loader.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    criterion : loss function (e.g., MSELoss)\n",
    "    scheduler : learning rate scheduler\n",
    "    num_epochs : int\n",
    "        Number of training epochs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_losses : list of float\n",
    "        Average training loss per epoch.\n",
    "    val_losses : list of float\n",
    "        Average validation loss per epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if torch.isnan(data.x).any() or torch.isnan(data.y).any():\n",
    "                continue\n",
    "\n",
    "            # Forward pass (with or without edge weights)\n",
    "            if use_edge_weights:\n",
    "                out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            else:\n",
    "                out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "            loss = criterion(out.view(-1, 1), data.y.view(-1, 1))\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # === Validation ===\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                if use_edge_weights:\n",
    "                    val_out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                else:\n",
    "                    val_out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "                val_loss = criterion(val_out.view(-1, 1), data.y.view(-1, 1)).item()\n",
    "                total_val_loss += val_loss\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d5068-939e-4ca2-8c38-c94182561153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Utilities\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def plot_loss_curves(train_losses, val_losses, title):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss per epoch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_fold_loss_curves(fold_loss_curves, segment_name):\n",
    "    \"\"\"\n",
    "    Plot loss curves for each fold separately.\n",
    "    \"\"\"\n",
    "    for animal_id, (train_losses, val_losses) in fold_loss_curves.items():\n",
    "        plot_loss_curves(train_losses, val_losses, f\"{segment_name} - Fold Animal {animal_id}\")\n",
    "\n",
    "def plot_average_loss_curves(fold_loss_curves, segment_name):\n",
    "    \"\"\"\n",
    "    Plot average loss curves across folds.\n",
    "    \"\"\"\n",
    "    all_train = [v[0] for v in fold_loss_curves.values()]\n",
    "    all_val = [v[1] for v in fold_loss_curves.values()]\n",
    "    avg_train = np.mean(np.stack(all_train), axis=0)\n",
    "    avg_val = np.mean(np.stack(all_val), axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(avg_train, label='Avg Train Loss')\n",
    "    plt.plot(avg_val, label='Avg Val Loss')\n",
    "    plt.title(f\"{segment_name} - Avg Loss Curves\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediction_vs_actual(loader, model, title, add_reference_line=True):\n",
    "    \"\"\"\n",
    "    Plots predicted vs. actual freezing % across all samples.\n",
    "    Colors indicate group identity (0 = black, 1 = pink, 2 = teal).\n",
    "    Automatically adapts to whether edge weights are used.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds, trues, groups = [], [], []\n",
    "    cmap = {0: 'black', 1: 'pink', 2: 'teal'}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if use_edge_weights:\n",
    "                out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            else:\n",
    "                out = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(out.item())\n",
    "            trues.append(data.y.item())\n",
    "            groups.append(data.group.item())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for grp in np.unique(groups):\n",
    "        idxs = [i for i, g in enumerate(groups) if g == grp]\n",
    "        plt.scatter(np.array(preds)[idxs], np.array(trues)[idxs], \n",
    "                    color=cmap.get(grp, 'gray'), label=f'Group {grp}', s=50, alpha=0.7)\n",
    "\n",
    "    if add_reference_line:\n",
    "        plt.plot([0, 100], [0, 100], 'r--', label='Perfect Prediction')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Freezing %\")\n",
    "    plt.ylabel(\"Actual Freezing %\")\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4ffea-9586-4fec-afc5-2737598f7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOCV Training Function\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "def loocv_training(graphs, best_params, num_features=6, num_epochs=110, segment_name=''):\n",
    "    \"\"\"\n",
    "    Performs Leave-One-Out Cross-Validation (LOOCV) using one graph (animal) as validation and the rest as training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graphs : list of Data\n",
    "        One PyG graph per animal.\n",
    "    best_params : dict\n",
    "        Hyperparameters from Optuna or manual config.\n",
    "    num_features : int\n",
    "        Node feature dimensionality.\n",
    "    num_epochs : int\n",
    "        Epochs per fold.\n",
    "    segment_name : str\n",
    "        For diagnostic printing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_train_loss : float\n",
    "    avg_val_loss : float\n",
    "    r2 : float\n",
    "        Final LOOCV RÂ² score.\n",
    "    fold_loss_curves : dict\n",
    "        Per-fold training and validation losses.\n",
    "    \"\"\"\n",
    "    loo = LeaveOneOut()\n",
    "    indices = np.arange(len(graphs))\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    animal_ids = []\n",
    "    fold_loss_curves = {}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(loo.split(indices)):\n",
    "        train_graphs = [graphs[i] for i in train_idx]\n",
    "        val_graphs = [graphs[i] for i in val_idx]\n",
    "        animal_id = val_graphs[0].unique_id.item()\n",
    "\n",
    "        print(f\"Fold {fold+1}/{len(graphs)}: Testing animal {animal_id}\")\n",
    "\n",
    "        train_loader = DataLoader(train_graphs, batch_size=1, shuffle=True, follow_batch=['x'])\n",
    "        val_loader = DataLoader(val_graphs, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "        model = create_model(\n",
    "            num_features=num_features,\n",
    "            hidden_channels=best_params['hidden_channels'],\n",
    "            dropout_rate=best_params['dropout_rate'],\n",
    "            heads=best_params['heads']\n",
    "        )\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs)\n",
    "        fold_loss_curves[animal_id] = (train_losses, val_losses)\n",
    "\n",
    "        # Final prediction on test animal\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                if use_edge_weights:\n",
    "                    pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                else:\n",
    "                    pred = model(data.x, data.edge_index, data.batch)\n",
    "                all_preds.append(pred.item())\n",
    "                all_targets.append(data.y.item())\n",
    "                animal_ids.append(animal_id)\n",
    "\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred = np.array(all_preds)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"LOOCV Summary ({segment_name})\")\n",
    "    print(f\"Avg Train Loss: {np.mean([np.mean(v[0]) for v in fold_loss_curves.values()]):.4f}\")\n",
    "    print(f\"Avg Val Loss: {np.mean([np.mean(v[1]) for v in fold_loss_curves.values()]):.4f}\")\n",
    "    print(f\"Aggregated RÂ²: {r2:.4f}\")\n",
    "\n",
    "    for a, p, t in zip(animal_ids, y_pred, y_true):\n",
    "        print(f\"Animal {a}: Predicted = {p:.2f}, Actual = {t:.2f}\")\n",
    "\n",
    "    return (\n",
    "        np.mean([np.mean(v[0]) for v in fold_loss_curves.values()]),  # Avg train loss\n",
    "        np.mean([np.mean(v[1]) for v in fold_loss_curves.values()]),  # Avg val loss\n",
    "        r2,\n",
    "        fold_loss_curves\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fa655-b36b-42ba-a150-dc7e3bbcdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Tuning Using K-Fold (Optional)\n",
    "# =====================================\n",
    "# Uses 5-fold CV to tune model hyperparameters.\n",
    "# Compatible with both GAT and edge-aware GAT based on `use_edge_weights` toggle.\n",
    "\n",
    "# === Define Search Space and Sampler ===\n",
    "search_space = {\n",
    "    'hidden_channels': [64, 128],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'heads': [1, 2, 4]\n",
    "}\n",
    "sampler = GridSampler(search_space)\n",
    "\n",
    "def objective_factory(graphs):\n",
    "    \"\"\"\n",
    "    Creates an Optuna objective function using KFold CV on the given graphs.\n",
    "    Returns a callable that Optuna can optimize.\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        # Sample hyperparameters\n",
    "        params = {\n",
    "            'hidden_channels': trial.suggest_categorical('hidden_channels', search_space['hidden_channels']),\n",
    "            'dropout_rate': trial.suggest_categorical('dropout_rate', search_space['dropout_rate']),\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', search_space['learning_rate']),\n",
    "            'heads': trial.suggest_int('heads', min(search_space['heads']), max(search_space['heads']))\n",
    "        }\n",
    "\n",
    "        kf = KFold(n_splits=5)\n",
    "        losses, r2s = [], []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(graphs):\n",
    "            train_loader = DataLoader([graphs[i] for i in train_idx], batch_size=1, shuffle=True)\n",
    "            val_loader = DataLoader([graphs[i] for i in val_idx], batch_size=1, shuffle=False)\n",
    "\n",
    "            model = create_model(num_features=6, **params)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=1e-5)\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            _, val_losses = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=110)\n",
    "\n",
    "            preds, trues = [], []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    if use_edge_weights:\n",
    "                        pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "                    else:\n",
    "                        pred = model(data.x, data.edge_index, data.batch)\n",
    "                    preds.append(pred.item())\n",
    "                    trues.append(data.y.item())\n",
    "\n",
    "            r2 = r2_score(trues, preds)\n",
    "            losses.append(np.mean(val_losses))\n",
    "            r2s.append(r2)\n",
    "\n",
    "        # Combined metric: low loss + high RÂ²\n",
    "        return np.mean(losses) - np.mean(r2s)\n",
    "\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd140317-0c30-43fe-ba31-baad278afa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GAT LOOCV FINAL EXECUTION BLOCK\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # === Set Your Data Paths ===\n",
    "    neuron_features_path = r\"path/to/neuron_features.xlsx\"\n",
    "    edge_weights_path = r\"path/to/edge_weights.xlsx\"\n",
    "    labels_path = r\"path/to/labels.xlsx\"\n",
    "\n",
    "    # === Load and Process Data ===\n",
    "    neuron_df, edge_df, labels_df = load_data(neuron_features_path, edge_weights_path, labels_path)\n",
    "    graphs_first600, graphs_last600 = create_graph_objects(neuron_df, edge_df, labels_df)\n",
    "    check_dummy_node_edges(graphs_first600 + graphs_last600)\n",
    "    graphs_first600 = normalize_features(graphs_first600)\n",
    "    graphs_last600 = normalize_features(graphs_last600)\n",
    "\n",
    "    # === Toggle Hyperparameter Search ===\n",
    "    run_optuna = False\n",
    "\n",
    "    if run_optuna:\n",
    "        print(\"ðŸ”Ž Running Optuna for First 600s\")\n",
    "        study_first = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "        study_first.optimize(objective_factory(graphs_first600))\n",
    "        best_params_first600 = study_first.best_params\n",
    "        print(\"âœ… Best Params First 600s:\", best_params_first600)\n",
    "\n",
    "        print(\"ðŸ”Ž Running Optuna for Last 600s\")\n",
    "        study_last = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "        study_last.optimize(objective_factory(graphs_last600))\n",
    "        best_params_last600 = study_last.best_params\n",
    "        print(\"âœ… Best Params Last 600s:\", best_params_last600)\n",
    "    else:\n",
    "        # Predefined best parameters (from previous tuning)\n",
    "        best_params_first600 = {'hidden_channels': 64, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'heads': 2}\n",
    "        best_params_last600 = {'hidden_channels': 128, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'heads': 1}\n",
    "\n",
    "    # === Run LOOCV for First 600s ===\n",
    "    print(\"\\nðŸš€ LOOCV Evaluation â€” First 600s\")\n",
    "    train_loss_1, val_loss_1, r2_1, curves_1 = loocv_training(\n",
    "        graphs_first600, best_params_first600, segment_name=\"First 600s\"\n",
    "    )\n",
    "    plot_average_loss_curves(curves_1, \"First 600s\")\n",
    "\n",
    "    # === Run LOOCV for Last 600s ===\n",
    "    print(\"\\nðŸš€ LOOCV Evaluation â€” Last 600s\")\n",
    "    train_loss_2, val_loss_2, r2_2, curves_2 = loocv_training(\n",
    "        graphs_last600, best_params_last600, segment_name=\"Last 600s\"\n",
    "    )\n",
    "    plot_average_loss_curves(curves_2, \"Last 600s\")\n",
    "\n",
    "    # === Plot Final Predictions ===\n",
    "    loader_first = DataLoader(graphs_first600, batch_size=1, shuffle=False)\n",
    "    loader_last = DataLoader(graphs_last600, batch_size=1, shuffle=False)\n",
    "\n",
    "    model_first = create_model(num_features=6, **best_params_first600)\n",
    "    model_last = create_model(num_features=6, **best_params_last600)\n",
    "\n",
    "    # Retrain on full data for prediction visualization\n",
    "    optimizer_f = optim.Adam(model_first.parameters(), lr=best_params_first600['learning_rate'], weight_decay=1e-5)\n",
    "    scheduler_f = CosineAnnealingLR(optimizer_f, T_max=50)\n",
    "    criterion_f = nn.MSELoss()\n",
    "    _ = train_model(model_first, loader_first, loader_first, optimizer_f, criterion_f, scheduler_f, num_epochs=110)\n",
    "\n",
    "    optimizer_l = optim.Adam(model_last.parameters(), lr=best_params_last600['learning_rate'], weight_decay=1e-5)\n",
    "    scheduler_l = CosineAnnealingLR(optimizer_l, T_max=50)\n",
    "    criterion_l = nn.MSELoss()\n",
    "    _ = train_model(model_last, loader_last, loader_last, optimizer_l, criterion_l, scheduler_l, num_epochs=110)\n",
    "\n",
    "    # Plot\n",
    "    plot_prediction_vs_actual(loader_first, model_first, title=\"First 600s: Predicted vs Actual\")\n",
    "    plot_prediction_vs_actual(loader_last, model_last, title=\"Last 600s: Predicted vs Actual\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch22)",
   "language": "python",
   "name": "pytorch22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
