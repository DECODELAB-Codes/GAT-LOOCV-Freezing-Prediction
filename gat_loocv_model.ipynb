{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec37be-1573-484e-a11e-5bfa24fe70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GAT LOOCV MODEL - NEURAL GRAPH LEARNING FOR BEHAVIOR PREDICTION\n",
    "# ============================================================\n",
    "# Predicts animal-level freezing behavior using neuron-level features and \n",
    "# functional connectivity (edge weights) in a GAT neural network framework.\n",
    "# ============================================================\n",
    "\n",
    "# === Core Libraries ===\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Torch + GNN Frameworks ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "\n",
    "# === Preprocessing + Evaluation ===\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# === Optuna for Hyperparameter Tuning ===\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler\n",
    "\n",
    "# === Reproducibility (optional) ===\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8ff6f-b86f-43d4-9bae-03b21856ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load neuron features, edge weights, and freezing labels\n",
    "def load_data(neuron_features_path, edge_weights_path, labels_path):\n",
    "    \"\"\"\n",
    "    Loads and validates neuron-level features, edge weights, and labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    neuron_features_path : str\n",
    "        Path to Excel file with neuron-level features.\n",
    "\n",
    "    edge_weights_path : str\n",
    "        Path to Excel file with neuron pair edge weights.\n",
    "\n",
    "    labels_path : str\n",
    "        Path to Excel file with animal-level freezing % and group.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    neuron_features_df, edge_weights_df, labels_df : pd.DataFrame\n",
    "        Loaded dataframes for feature, edge, and label information.\n",
    "    \"\"\"\n",
    "    neuron_features_df = pd.read_excel(neuron_features_path)\n",
    "    edge_weights_df = pd.read_excel(edge_weights_path)\n",
    "    labels_df = pd.read_excel(labels_path)\n",
    "\n",
    "    # Fix column naming inconsistencies\n",
    "    neuron_features_df.rename(columns={'n_clust': 'Neuron'}, inplace=True)\n",
    "\n",
    "    # Convert neuron pair strings like \"(1, 2)\" to tuples\n",
    "    def convert_pair(pair_str):\n",
    "        if 'dummy' in pair_str.lower():\n",
    "            return (0, 1)\n",
    "        nums = re.findall(r'\\d+', pair_str)\n",
    "        return tuple(map(int, nums))\n",
    "\n",
    "    edge_weights_df['Neuron Pair'] = edge_weights_df['Neuron Pair'].apply(convert_pair)\n",
    "\n",
    "    # Validation checks\n",
    "    assert set(neuron_features_df['Unique ID'].unique()).issubset(edge_weights_df['Unique ID'].unique()), \\\n",
    "        \"Mismatch in Unique IDs between neuron features and edge weights.\"\n",
    "    assert set(neuron_features_df['Unique ID'].unique()).issubset(labels_df['UniqueID'].unique()), \\\n",
    "        \"Mismatch in Unique IDs between neuron features and labels.\"\n",
    "\n",
    "    return neuron_features_df, edge_weights_df, labels_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b1d8f-f269-41a2-9fe0-63730030bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert per-animal data into PyG graph objects\n",
    "\n",
    "def create_graph_objects(neuron_features_df, edge_weights_df, labels_df):\n",
    "    \"\"\"\n",
    "    Constructs torch_geometric Data objects (one per animal) using:\n",
    "    - node features (neurons)\n",
    "    - edge index (connectivity)\n",
    "    - edge attributes (weights)\n",
    "    - label (freezing %)\n",
    "    - group and unique ID metadata\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    animal_graphs_first600 : list of torch_geometric.data.Data\n",
    "    animal_graphs_last600 : list of torch_geometric.data.Data\n",
    "    \"\"\"\n",
    "\n",
    "    animal_graphs_first600 = []\n",
    "    animal_graphs_last600 = []\n",
    "\n",
    "    for unique_id in neuron_features_df['Unique ID'].unique():\n",
    "        neurons = neuron_features_df[neuron_features_df['Unique ID'] == unique_id]\n",
    "        edges = edge_weights_df[edge_weights_df['Unique ID'] == unique_id]\n",
    "        label_row = labels_df[labels_df['UniqueID'] == unique_id]\n",
    "\n",
    "        if neurons.empty or edges.empty or label_row.empty:\n",
    "            print(f\"⚠️ Missing data for animal {unique_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        group = label_row['Group'].iloc[0]\n",
    "\n",
    "        # Feature selection\n",
    "        feat_f600 = ['Firing rate first 600', 'mISI (s) first 600', 'maxISI (s) first 600',\n",
    "                     'minISI (s) first 600', 'CVISI first 600', 'PC1score whole', 'PC2score whole']\n",
    "        feat_l600 = ['Firing rate last 600', 'mISI (s) last 600', 'maxISI (s) last 600',\n",
    "                     'minISI (s) last 600', 'CVISI last 600', 'PC1score whole', 'PC2score whole']\n",
    "\n",
    "        x_f600 = torch.tensor(neurons[feat_f600].values, dtype=torch.float)\n",
    "        x_l600 = torch.tensor(neurons[feat_l600].values, dtype=torch.float)\n",
    "\n",
    "        # Edge index (convert 1-indexed → 0-indexed)\n",
    "        edge_index = torch.tensor([(i-1, j-1) for i, j in edges['Neuron Pair']], dtype=torch.long).t().contiguous()\n",
    "        edge_attr_f600 = torch.tensor(edges['Mean Edge Weight First 600s'].values, dtype=torch.float).view(-1, 1)\n",
    "        edge_attr_l600 = torch.tensor(edges['Mean Edge Weight Last 600s'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        y_f600 = torch.tensor(label_row['Percentage Freezing First 600'].values, dtype=torch.float)\n",
    "        y_l600 = torch.tensor(label_row['Percentage Freezing Last 600'].values, dtype=torch.float)\n",
    "\n",
    "        data_f600 = Data(x=x_f600, edge_index=edge_index, edge_attr=edge_attr_f600,\n",
    "                         y=y_f600, group=torch.tensor([group]), unique_id=torch.tensor([unique_id]))\n",
    "        data_l600 = Data(x=x_l600, edge_index=edge_index, edge_attr=edge_attr_l600,\n",
    "                         y=y_l600, group=torch.tensor([group]), unique_id=torch.tensor([unique_id]))\n",
    "\n",
    "        animal_graphs_first600.append(data_f600)\n",
    "        animal_graphs_last600.append(data_l600)\n",
    "\n",
    "    print(f\"✅ Created {len(animal_graphs_first600)} graphs for First 600s\")\n",
    "    print(f\"✅ Created {len(animal_graphs_last600)} graphs for Last 600s\")\n",
    "\n",
    "    return animal_graphs_first600, animal_graphs_last600\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c9926-77c4-44f9-aca2-8bc3ce742407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety checks on edge index values\n",
    "\n",
    "def check_dummy_node_edges(graphs):\n",
    "    \"\"\"\n",
    "    Sanity check for edge index issues.\n",
    "    Any edge indices < 0 are corrected to the last valid node index.\n",
    "    \"\"\"\n",
    "    for graph in graphs:\n",
    "        eid = graph.unique_id.item()\n",
    "        if graph.edge_index.min() < 0:\n",
    "            print(f\"⚠️ Edge index < 0 found in animal {eid}. Attempting fix.\")\n",
    "            corrected = graph.edge_index.clone()\n",
    "            corrected[corrected < 0] = graph.num_nodes - 1\n",
    "            graph.edge_index = corrected\n",
    "            print(f\"✅ Edge correction applied for animal {eid}.\")\n",
    "        else:\n",
    "            print(f\"✅ Edge index OK for animal {eid}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479ae2b-5f84-40ef-82bb-59550649930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler to features\n",
    "\n",
    "def normalize_features(graphs):\n",
    "    \"\"\"\n",
    "    Applies sklearn StandardScaler to node features within each graph.\n",
    "    Normalizes features to zero mean and unit variance.\n",
    "\n",
    "    This is done graph-by-graph to preserve each animal’s independence.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    for graph in graphs:\n",
    "        graph.x = torch.tensor(scaler.fit_transform(graph.x), dtype=torch.float)\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3be24b-83d0-440f-a337-e438172d8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define GAT Model\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network (GAT) with:\n",
    "    - 3 attention layers\n",
    "    - Sigmoid output scaled to 0–100 (freezing %)\n",
    "    - Uses PyTorch Geometric GATConv\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, hidden_channels, dropout_rate=0.3, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels * heads, 1, heads=1)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)  # Aggregate to graph-level output\n",
    "        return torch.sigmoid(x) * 100  # Scale to match 0–100% freezing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fed4df-0000-474a-bd65-5bb74a4fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train GAT Model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=110):\n",
    "    \"\"\"\n",
    "    Trains the GAT model and records loss per epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "    train_loader : DataLoader\n",
    "    val_loader : DataLoader\n",
    "    optimizer : torch.optim.Optimizer\n",
    "    criterion : loss function\n",
    "    scheduler : learning rate scheduler\n",
    "    num_epochs : int\n",
    "        Number of training epochs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_losses : list\n",
    "    val_losses : list\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Defensive check for NaNs\n",
    "            if torch.isnan(data.x).any() or torch.isnan(data.y).any():\n",
    "                continue\n",
    "\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out.view(-1, 1), data.y.view(-1, 1))\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(total_train_loss / len(train_loader.dataset))\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                out = model(data.x, data.edge_index, data.batch)\n",
    "                val_loss = criterion(out.view(-1, 1), data.y.view(-1, 1)).item()\n",
    "                total_val_loss += val_loss\n",
    "        val_losses.append(total_val_loss / len(val_loader.dataset))\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "\n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d5068-939e-4ca2-8c38-c94182561153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Helpers\n",
    "\n",
    "def plot_loss_curves(train_losses, val_losses, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_fold_loss_curves(fold_loss_curves, segment_name):\n",
    "    for animal_id, (train_losses, val_losses) in fold_loss_curves.items():\n",
    "        plot_loss_curves(train_losses, val_losses, f\"{segment_name} - Fold Animal {animal_id}\")\n",
    "\n",
    "def plot_average_loss_curves(fold_loss_curves, segment_name):\n",
    "    all_train = [v[0] for v in fold_loss_curves.values()]\n",
    "    all_val = [v[1] for v in fold_loss_curves.values()]\n",
    "    avg_train = np.mean(np.stack(all_train), axis=0)\n",
    "    avg_val = np.mean(np.stack(all_val), axis=0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(avg_train, label='Avg Train Loss')\n",
    "    plt.plot(avg_val, label='Avg Val Loss')\n",
    "    plt.title(f\"{segment_name} - Avg Loss Curves\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_prediction_vs_actual(loader, model, title, add_reference_line=True):\n",
    "    \"\"\"\n",
    "    Plots predicted vs. actual freezing %.\n",
    "    Group colors: 0=black, 1=pink, 2=teal.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds, trues, groups = [], [], []\n",
    "    cmap = {0: 'black', 1: 'pink', 2: 'teal'}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            preds.append(out.item())\n",
    "            trues.append(data.y.item())\n",
    "            groups.append(data.group.item())\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for grp in np.unique(groups):\n",
    "        idxs = [i for i, g in enumerate(groups) if g == grp]\n",
    "        plt.scatter(np.array(preds)[idxs], np.array(trues)[idxs], \n",
    "                    color=cmap.get(grp, 'gray'), label=f'Group {grp}', s=50, alpha=0.7)\n",
    "\n",
    "    if add_reference_line:\n",
    "        plt.plot([0, 100], [0, 100], 'r--', label='Perfect Prediction')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted Freezing %\")\n",
    "    plt.ylabel(\"Actual Freezing %\")\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4ffea-9586-4fec-afc5-2737598f7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOOCV\n",
    "\n",
    "def loocv_training(graphs, best_params, num_features=6, num_epochs=110, segment_name=''):\n",
    "    \"\"\"\n",
    "    Runs LOOCV using one graph (animal) as validation and the rest as training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graphs : list of Data\n",
    "        Graphs, one per animal.\n",
    "    best_params : dict\n",
    "        Dictionary of hyperparameters from Optuna or manual selection.\n",
    "    num_features : int\n",
    "        Number of features per node.\n",
    "    num_epochs : int\n",
    "        Epochs to train each fold.\n",
    "    segment_name : str\n",
    "        Used in plot titles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    avg_train_loss, avg_val_loss, R², fold_loss_curves\n",
    "    \"\"\"\n",
    "    loo = LeaveOneOut()\n",
    "    indices = np.arange(len(graphs))\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    animal_ids = []\n",
    "    fold_loss_curves = {}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(loo.split(indices)):\n",
    "        train_graphs = [graphs[i] for i in train_idx]\n",
    "        val_graphs = [graphs[i] for i in val_idx]\n",
    "        animal_id = val_graphs[0].unique_id.item()\n",
    "\n",
    "        print(f\"\\n🔍 Fold {fold+1}/{len(graphs)}: Testing animal {animal_id}\")\n",
    "\n",
    "        train_loader = DataLoader(train_graphs, batch_size=1, shuffle=True, follow_batch=['x'])\n",
    "        val_loader = DataLoader(val_graphs, batch_size=1, shuffle=False, follow_batch=['x'])\n",
    "\n",
    "        model = GAT(\n",
    "            num_features=num_features,\n",
    "            hidden_channels=best_params['hidden_channels'],\n",
    "            dropout_rate=best_params['dropout_rate'],\n",
    "            heads=best_params['heads']\n",
    "        )\n",
    "        optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'], weight_decay=1e-5)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs)\n",
    "        fold_loss_curves[animal_id] = (train_losses, val_losses)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                pred = model(data.x, data.edge_index, data.batch)\n",
    "                all_preds.append(pred.item())\n",
    "                all_targets.append(data.y.item())\n",
    "                animal_ids.append(animal_id)\n",
    "\n",
    "    # Compute aggregated R²\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred = np.array(all_preds)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n📊 LOOCV Summary ({segment_name})\")\n",
    "    print(f\"Avg Train Loss: {np.mean([np.mean(v[0]) for v in fold_loss_curves.values()]):.4f}\")\n",
    "    print(f\"Avg Val Loss: {np.mean([np.mean(v[1]) for v in fold_loss_curves.values()]):.4f}\")\n",
    "    print(f\"Aggregated R²: {r2:.4f}\")\n",
    "\n",
    "    for a, p, t in zip(animal_ids, y_pred, y_true):\n",
    "        print(f\"Animal {a}: Predicted = {p:.2f}, Actual = {t:.2f}\")\n",
    "\n",
    "    return (\n",
    "        np.mean([np.mean(v[0]) for v in fold_loss_curves.values()]),  # Avg train loss\n",
    "        np.mean([np.mean(v[1]) for v in fold_loss_curves.values()]),  # Avg val loss\n",
    "        r2,\n",
    "        fold_loss_curves\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fa655-b36b-42ba-a150-dc7e3bbcdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optuna Tuning Using Kfold (Optional). Optuna: Uses K-Fold (k=5) to tune hyperparameters\n",
    "\n",
    "# Grid Search Space for Optuna\n",
    "search_space = {\n",
    "    'hidden_channels': [64, 128],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'heads': [1, 2, 4]\n",
    "}\n",
    "sampler = GridSampler(search_space)\n",
    "\n",
    "def objective_factory(graphs):\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'hidden_channels': trial.suggest_categorical('hidden_channels', search_space['hidden_channels']),\n",
    "            'dropout_rate': trial.suggest_categorical('dropout_rate', search_space['dropout_rate']),\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', search_space['learning_rate']),\n",
    "            'heads': trial.suggest_int('heads', min(search_space['heads']), max(search_space['heads']))\n",
    "        }\n",
    "\n",
    "        kf = KFold(n_splits=5)\n",
    "        losses, r2s = [], []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(graphs):\n",
    "            train_loader = DataLoader([graphs[i] for i in train_idx], batch_size=1, shuffle=True)\n",
    "            val_loader = DataLoader([graphs[i] for i in val_idx], batch_size=1, shuffle=False)\n",
    "\n",
    "            model = GAT(num_features=6, **params)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=1e-5)\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=50)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            _, val_losses = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=110)\n",
    "\n",
    "            preds, trues = [], []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    preds.append(model(data.x, data.edge_index, data.batch).item())\n",
    "                    trues.append(data.y.item())\n",
    "\n",
    "            r2 = r2_score(trues, preds)\n",
    "            losses.append(np.mean(val_losses))\n",
    "            r2s.append(r2)\n",
    "\n",
    "        return np.mean(losses) - np.mean(r2s)\n",
    "    return objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd140317-0c30-43fe-ba31-baad278afa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Execution. Final model: Uses LOOCV to evaluate prediction performance\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # === Set Your Data Paths Here ===\n",
    "    neuron_features_path = r\"path/to/neuron_features.xlsx\"\n",
    "    edge_weights_path = r\"path/to/edge_weights.xlsx\"\n",
    "    labels_path = r\"path/to/labels.xlsx\"\n",
    "\n",
    "    # === Load and Prepare Data ===\n",
    "    neuron_df, edge_df, labels_df = load_data(neuron_features_path, edge_weights_path, labels_path)\n",
    "    graphs_first600, graphs_last600 = create_graph_objects(neuron_df, edge_df, labels_df)\n",
    "    check_dummy_node_edges(graphs_first600 + graphs_last600)\n",
    "    graphs_first600 = normalize_features(graphs_first600)\n",
    "    graphs_last600 = normalize_features(graphs_last600)\n",
    "\n",
    "    # === Toggle Optuna Hyperparameter Search (Optional) ===\n",
    "    run_optuna = False\n",
    "\n",
    "    if run_optuna:\n",
    "        study_first = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "        study_first.optimize(objective_factory(graphs_first600))\n",
    "        best_params_first600 = study_first.best_params\n",
    "        print(\"Best Params First 600s:\", best_params_first600)\n",
    "\n",
    "        study_last = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "        study_last.optimize(objective_factory(graphs_last600))\n",
    "        best_params_last600 = study_last.best_params\n",
    "        print(\"Best Params Last 600s:\", best_params_last600)\n",
    "    else:\n",
    "        best_params_first600 = {'hidden_channels': 64, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'heads': 2}\n",
    "        best_params_last600 = {'hidden_channels': 128, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'heads': 1}\n",
    "\n",
    "    # === LOOCV Evaluation ===\n",
    "    print(\"\\n=== First 600s ===\")\n",
    "    train_loss, val_loss, r2, curves = loocv_training(graphs_first600, best_params_first600, segment_name=\"First 600s\")\n",
    "    plot_average_loss_curves(curves, \"First 600s\")\n",
    "\n",
    "    print(\"\\n=== Last 600s ===\")\n",
    "    train_loss, val_loss, r2, curves = loocv_training(graphs_last600, best_params_last600, segment_name=\"Last 600s\")\n",
    "    plot_average_loss_curves(curves, \"Last 600s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch22)",
   "language": "python",
   "name": "pytorch22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
